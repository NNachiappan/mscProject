{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9425d228",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!Reference!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "Code from Wong et al.\n",
    "\n",
    "https://github.com/alexklwong/targeted-adversarial-perturbations-monocular-depth\n",
    "\n",
    "\n",
    "and Godard et al.\n",
    "\n",
    "https://github.com/mrharicot/monodepth\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'onedrive/MSc Project/TAP-MD/src/run_perturb.py' \\\n",
    "--image_path 'onedrive/MSc Project/KITTI/utils/kitti_eigen_test_image0.txt' \\\n",
    "--n_height 192 \\\n",
    "--n_width 640 \\\n",
    "--n_channel 3 \\\n",
    "--output_norm 0.02 \\\n",
    "--n_step 250 \\\n",
    "--learning_rates 4.0 1.0 \\\n",
    "--learning_schedule 400 \\\n",
    "--depth_method monodepth \\\n",
    "--depth_transform_func multiply \\\n",
    "--depth_transform_value 1.10 \\\n",
    "--mask_constraint none \\\n",
    "--checkpoint_path perturbations/monodepth/all_mult110_norm002_lr4e0_1e0_plain \\\n",
    "--depth_model_restore_path0 local/modelweights/plain/encoder-60.pth \\\n",
    "--depth_model_restore_path1 local/modelweights/plain/decoder-60.pth \\\n",
    "--device gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70893893",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'onedrive/MSc Project/TAP-MD/src/run_perturb.py' \\\n",
    "--image_path 'onedrive/MSc Project/KITTI/utils/kitti_eigen_test_image0.txt' \\\n",
    "--n_height 192 \\\n",
    "--n_width 640 \\\n",
    "--n_channel 3 \\\n",
    "--output_norm 0.02 \\\n",
    "--n_step 250 \\\n",
    "--learning_rates 4.0 1.0 \\\n",
    "--learning_schedule 400 \\\n",
    "--depth_method monodepth \\\n",
    "--depth_transform_func multiply \\\n",
    "--depth_transform_value 1.10 \\\n",
    "--mask_constraint none \\\n",
    "--checkpoint_path perturbations/monodepth/all_mult110_norm002_lr4e0_1e0_adv \\\n",
    "--depth_model_restore_path0 local/modelweights/adv/encoder-100.pth \\\n",
    "--depth_model_restore_path1 local/modelweights/adv/decoder-100.pth \\\n",
    "--device gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d52965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'onedrive/MSc Project/TAP-MD/src/run_perturb.py' \\\n",
    "--image_path 'onedrive/MSc Project/KITTI/utils/kitti_eigen_test_image0.txt' \\\n",
    "--n_height 192 \\\n",
    "--n_width 640 \\\n",
    "--n_channel 3 \\\n",
    "--output_norm 0.02 \\\n",
    "--n_step 250 \\\n",
    "--learning_rates 4.0 1.0 \\\n",
    "--learning_schedule 400 \\\n",
    "--depth_method monodepth \\\n",
    "--depth_transform_func multiply \\\n",
    "--depth_transform_value 1.10 \\\n",
    "--mask_constraint none \\\n",
    "--checkpoint_path perturbations/monodepth/all_mult110_norm002_lr4e0_1e0_mixed \\\n",
    "--depth_model_restore_path0 local/modelweights/half/encoder-100.pth \\\n",
    "--depth_model_restore_path1 local/modelweights/half/decoder-100.pth \\\n",
    "--device gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/alexklwong/targeted-adversarial-perturbations-monocular-depth.git\n",
    "!mv targeted-adversarial-perturbations-monocular-depth/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00502b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('external_src/monodepth/src')\n",
    "import datasets, data_utils\n",
    "import global_constants as settings\n",
    "from log_utils import log\n",
    "from monodepth_model import MonodepthModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(gt, pred):\n",
    "    thresh = np.maximum((gt / pred), (pred / gt))\n",
    "    a1 = (thresh < 1.25   ).mean()\n",
    "    a2 = (thresh < 1.25 ** 2).mean()\n",
    "    a3 = (thresh < 1.25 ** 3).mean()\n",
    "\n",
    "    rmse = (gt - pred) ** 2\n",
    "    rmse = np.sqrt(rmse.mean())\n",
    "\n",
    "    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n",
    "    rmse_log = np.sqrt(rmse_log.mean())\n",
    "\n",
    "    abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "\n",
    "    sq_rel = np.mean(((gt - pred)**2) / gt)\n",
    "\n",
    "    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model\n",
    "model = MonodepthModel(\n",
    "        encoder_type='resnet18',\n",
    "        decoder_type='up',\n",
    "        activation_func='elu',\n",
    "        n_pyramid=4,\n",
    "        scale_factor=0.30,\n",
    "        device='cuda')\n",
    "\n",
    "\n",
    "loaded_dict_encoder = torch.load('onedrive/MSc Project/monodepth_models/resnet18advmphalf/encoder-100.pth', map_location=model.device)\n",
    "loaded_dict_decoder = torch.load('onedrive/MSc Project/monodepth_models/resnet18advmphalf/decoder-100.pth', map_location=model.device)\n",
    "model.encoder.load_state_dict(loaded_dict_encoder['model_state_dict'])\n",
    "model.decoder.load_state_dict(loaded_dict_decoder['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ecfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test images\n",
    "image0_paths = data_utils.read_paths('onedrive/MSc Project/KITTI/utils/kitti_eigen_test_image0.txt')\n",
    "image1_paths = data_utils.read_paths('onedrive/MSc Project/KITTI/utils/kitti_eigen_test_image1.txt')\n",
    "camera_paths = data_utils.read_paths('onedrive/MSc Project/KITTI/utils/kitti_eigen_test_camera.txt')\n",
    "n_sample = len(image0_paths)\n",
    "\n",
    "\n",
    "for i in range(len(image0_paths)):\n",
    "    image0_paths[i] = \"local/trainingDataSet/\" + image0_paths[i]\n",
    "    image1_paths[i] = \"local/trainingDataSet/\" + image1_paths[i]\n",
    "    camera_paths[i] = \"local/trainingDataSet/\" + camera_paths[i]\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.ImagePairCameraDataset(\n",
    "        image0_paths,\n",
    "        image1_paths,\n",
    "        camera_paths,\n",
    "        shape=(192, 640)),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test plain model at 60th epoch.\n",
    "#FGSM\n",
    "import cv2\n",
    "\n",
    "modelName = 'plain'\n",
    "locNames = ['plain', 'adv', 'half']\n",
    "\n",
    "epochNums = [60, 100, 100]\n",
    "\n",
    "n_sample = len(image0_paths)\n",
    "\n",
    "gt_depths = np.load('gt_depths.npy')\n",
    "\n",
    "min_depth = 0\n",
    "max_depth = 80\n",
    "\n",
    "#Load Model\n",
    "model = MonodepthModel(\n",
    "        encoder_type='resnet18',\n",
    "        decoder_type='up',\n",
    "        activation_func='elu',\n",
    "        n_pyramid=4,\n",
    "        scale_factor=0.30,\n",
    "        device='cuda')\n",
    "\n",
    "for k in range(3):\n",
    "    \n",
    "    modelName = locNames[k]\n",
    "    epochNum = epochNums[k]\n",
    "    \n",
    "    loaded_dict_encoder = torch.load(\"local/modelweights/\"+ modelName +\"/encoder-\"+ str(epochNum) +\".pth\", map_location=model.device)\n",
    "    loaded_dict_decoder = torch.load(\"local/modelweights/\"+ modelName +\"/decoder-\"+ str(epochNum) +\".pth\", map_location=model.device)\n",
    "    model.encoder.load_state_dict(loaded_dict_encoder['model_state_dict'])\n",
    "    model.decoder.load_state_dict(loaded_dict_decoder['model_state_dict'])\n",
    "\n",
    "    rms     = np.zeros((2, n_sample), np.float32)\n",
    "    log_rms = np.zeros((2, n_sample), np.float32)\n",
    "    abs_rel = np.zeros((2, n_sample), np.float32)\n",
    "    sq_rel  = np.zeros((2, n_sample), np.float32)\n",
    "    a1      = np.zeros((2, n_sample), np.float32)\n",
    "    a2      = np.zeros((2, n_sample), np.float32)\n",
    "    a3      = np.zeros((2, n_sample), np.float32)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    parameters = model.parameters()\n",
    "    optimizer = torch.optim.SGD(parameters, lr=0.01)\n",
    "\n",
    "    for image0, image1, camera in dataloader:\n",
    "        print(\"\\r\" + str(i), end=\"\")\n",
    "        image0 = image0.cuda()\n",
    "        image1 = image1.cuda()\n",
    "        camera = camera.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         image0.requires_grad = True\n",
    "\n",
    "        pred_depth = model.forward(image0, camera)\n",
    "        pred_depth_orig = np.squeeze(pred_depth[2].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "#         #FGSM Vanilla Attack##################     \n",
    "#         loss = model.compute_loss(image0, image1,\n",
    "#                 w_color=settings.W_COLOR,\n",
    "#                 w_ssim=settings.W_SSIM,\n",
    "#                 w_smoothness=settings.W_SMOOTHNESS,\n",
    "#                 w_left_right=settings.W_LEFT_RIGHT)\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "\n",
    "#         #Get gradient of image\n",
    "#         img_grad = image0.grad.data\n",
    "#         #Sign\n",
    "#         sign_img_grad = img_grad.sign()\n",
    "#         epsilon = 8/255.\n",
    "#         #pertubation\n",
    "#         pertubation = epsilon*sign_img_grad\n",
    "#         pert_img = image0 + pertubation\n",
    "#         #Clip\n",
    "#         pert_img = torch.clamp(pert_img, 0, 1)\n",
    "#         ######################################\n",
    "\n",
    "        #PGD Attack##################\n",
    "        epsilon = 8/255.\n",
    "        alpha = 2/255.\n",
    "\n",
    "        delta = torch.zeros_like(image0).uniform_(-epsilon, epsilon).cuda()\n",
    "        advImage = image0 + delta\n",
    "        for k in range(7):\n",
    "\n",
    "            delta.requires_grad = True\n",
    "            advImage = image0 + delta\n",
    "            model.forward(advImage, camera)\n",
    "            loss = model.compute_loss(advImage, image1,\n",
    "                    w_color=settings.W_COLOR,\n",
    "                    w_ssim=settings.W_SSIM,\n",
    "                    w_smoothness=settings.W_SMOOTHNESS,\n",
    "                    w_left_right=settings.W_LEFT_RIGHT)\n",
    "            loss.backward()\n",
    "\n",
    "            delta_grad = delta.grad.detach()\n",
    "            delta.data = torch.clamp(delta + (alpha*torch.sign(delta_grad)), -epsilon, epsilon)\n",
    "            delta = delta.detach()\n",
    "\n",
    "            advImage = torch.clamp(advImage + delta, 0, 1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        pert_img = advImage\n",
    "        ######################################\n",
    "\n",
    "        pert_depth_all = model.forward(pert_img, camera)\n",
    "        pert_depth = np.squeeze(pert_depth_all[2].detach().cpu().numpy())\n",
    "        pert_depth_rescaled = cv2.resize(pert_depth, dsize=(1242, 375), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    #     if i % 50 == 0:\n",
    "    #         plt.imshow(np.transpose(np.squeeze(image0.detach().cpu().numpy()), (1, 2, 0)))\n",
    "    #         plt.show()\n",
    "    #         plt.imshow(np.squeeze(pred_depth[0].detach().cpu().numpy()))\n",
    "    #         plt.show()\n",
    "\n",
    "    #         pertubation = np.squeeze(delta.detach().cpu().numpy())\n",
    "    #         normPert = (pertubation - np.amin(pertubation)) / (np.amax(pertubation) - np.amin(pertubation))\n",
    "\n",
    "    #         plt.imshow(np.transpose(normPert, (1, 2, 0)))\n",
    "    #         plt.show()\n",
    "    #         plt.imshow(np.transpose(np.squeeze(pert_img.detach().cpu().numpy()), (1, 2, 0)))\n",
    "    #         plt.show()\n",
    "    #         plt.imshow(np.squeeze(pert_depth_all[0].detach().cpu().numpy()))\n",
    "    #         plt.show()\n",
    "\n",
    "\n",
    "        #Compare images\n",
    "        #Convert nan depths to max, as likey is to be inf depth due to 0 disparity.\n",
    "        pred_depth_orig = np.nan_to_num(pred_depth_orig, nan=max_depth)\n",
    "        pred_depth_orig[pred_depth_orig < min_depth] = min_depth\n",
    "        pred_depth_orig[pred_depth_orig > max_depth] = max_depth\n",
    "\n",
    "        pert_depth = np.nan_to_num(pert_depth, nan=max_depth)\n",
    "        pert_depth[pert_depth < min_depth] = min_depth\n",
    "        pert_depth[pert_depth > max_depth] = max_depth\n",
    "\n",
    "        abs_rel[0][i], sq_rel[0][i], rms[0][i], log_rms[0][i], a1[0][i], a2[0][i], a3[0][i] = compute_errors(pred_depth_orig, pert_depth)\n",
    "\n",
    "\n",
    "\n",
    "        gt_depth = gt_depths[i]\n",
    "\n",
    "        #Compute for only values inbetween 0 and 80 as the ground truth velodyne data is very sparse.\n",
    "        mask = np.logical_and(gt_depth > min_depth, gt_depth < max_depth)\n",
    "\n",
    "        pert_depth_rescaled = np.nan_to_num(pert_depth_rescaled, nan=max_depth)\n",
    "        pert_depth_rescaled[pert_depth_rescaled < min_depth] = min_depth\n",
    "        pert_depth_rescaled[pert_depth_rescaled > max_depth] = max_depth\n",
    "\n",
    "        abs_rel[1][i], sq_rel[1][i], rms[1][i], log_rms[1][i], a1[1][i], a2[1][i], a3[1][i] = compute_errors(gt_depth[mask], pert_depth_rescaled[mask])\n",
    "\n",
    "        i+=1\n",
    "    print(abs_rel.mean(axis=1))\n",
    "    print(sq_rel.mean(axis=1))\n",
    "    print(rms.mean(axis=1))\n",
    "    print(log_rms.mean(axis=1))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
